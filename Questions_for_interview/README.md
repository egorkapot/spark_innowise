<h1 align="center">Questions for interview</h1>


## Description

Здесь будут вопросы, которые могу вам попасться на собесе.

Если вы хотите чтобы ваш вопрос был освещен здесь - пишите мне в личку в тг @egorkapot или делайте мердж реквест

 
# Questions

## Что такое Spark

BigData фреймворк с открытым исходным кодом, для распределенной пакетной и потоковой обработки неструктурированных и слабоструктурированных данных, входящий в экосистему Hadoop. 

В устройстве систем лежит 6 понятий: 

-   Cluster – все вычислительные средства (компьютеры или сервера), которые называются **нодами**. Каждая нода имеет свое железо и ОС. Нодами в кластере управляет **cluster manager**. 

-   Worker - Нода или сеть нод ресурсы которых используются для работы спарка.  

-   Executor - процесс ранящийся в JVM. На каждом воркере может быть несколько **executors**. Именно **executor** выполняет работу и процессы используя железо воркера. 

-   Master - машина откуда пишется код на выполнение. По сути там где вы пишете код 

-   Driver - Процесс который в зависимости от запуска может быть как на мастере так и на ноде. В драйвере есть метод **main()** который создает **Spark-session** что является логическим центром спарка и делает следующие вещи: 

    -   Нарезает код на **job, stage, task**

    -   Создает **Logical & Physical Plan** 

    -   Координирует **cluster manager** для отправки задач на **executors** 

    -   Отслеживает процесс выполнения с его этапами 

-   Cluster Manager - отвечает за **выделяемые ресурсы**. Когда мы запускаем код то поднимается **driver**, который говорит сколько нужно executors с какими ресурсами для выполнения задачи и говорит об этом cluster manager. Есть несколько видов Cluster Manager: 

    -   Spark Standalone Cluster Manager. Самый обычный и поставляется с самим спарком. Не требует настройки)) 

    -   Apache Mesos. Чуть сложнее и круче, но если предыдущий используется в тестовых целях, то этот я вообще не видел чтобы юзался. 

    -   Hadoop YARN. До прихода на рынок k8s был одним из лучших решений, однако из-за особенностей устройства сейчас юзается реже, например в облачных решениях(тот же AWS EMR). 

    -   k8s. Самый лучший и чаще всего будет он. Об устройстве k8s можно почитать в интернете, я лишь скажу что в рамках спарка один executor=один pod. Это добавляет дополнительную гибкость. 

 
## Что такое Map Reduce и в чём его суть

Технология для параллельной обработки данных в распределённых кластерах. Программы автоматически распараллеливаются и выполняются на разных нодах кластера, при этом исполнительная система сама заботится о деталях реализации. Может использоваться много для чего: индексация веб контента, кластеризация документов, машинное обучение итд. За основу взяты две процедуры **map** - применяет нужную функцию к каждому элементу списка и **reduce** - объединяет результаты работы map. 


## Какие этапы есть в Map Reduce

-   Map – предварительная обработка входных данных в виде большого **списка** значений. При этом главный узел кластера (master node) получает этот список, **делит** его **на части** и передает рабочим узлам (worker node). Далее **каждый** рабочий **узел** применяет функцию **Map** к локальным данным и записывает результат в формате «**ключ-значение**» во **временное хранилище**. 

-   Combine - опциональный процесс, который ранится на каждой ноде **отдельно**. По сути это тот же редьюс до шафла который уменьшает количество данных которые будут закинуты в шафл 

-   Shuffle - когда рабочие узлы **перераспределяют** данные на основе **ключей**, ранее созданных функцией Map, таким образом, чтобы все **данные** одного **ключа** лежали на **одном** рабочем **узле**. 

-   Reduce – параллельная **обработка** каждым рабочим узлом каждой группы данных по порядку следования ключей и «**склейка**» результатов на **master node**. Главный узел получает промежуточные ответы от рабочих узлов и передаёт их на свободные узлы для выполнения следующего шага. Получившийся после прохождения всех необходимых шагов результат – это и есть решение исходной задачи. 


## Разница между Spark и Hadoop

Хадуповский MapReduce обрабатывает данные на базе **дискового хранилища**, в то время как спарк использует специальные примитивы для рекуррентной обработки в **оперативной памяти**. За счет этого вычислительные задачи реализуются на спарке значительно **быстрее**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/hadoopvsspark.png" width="80%"></p>

## RDD, DF, Dataset - разница между ними(в рамках PySpark только RDD и DF)

RDD - фундаментальная структура данных в Spark можно сказать low-level API. Стоит запомнить что RDD:

    -   Неизменяем
    -   Распределен (много партиций)
    -   Партиции RDD обрабатываются параллельно
    -   Fault-tolerant (автоматически восстанавливаются при падении ноды)


Dataframe и Dataset были построены на механизме RDD и получили новые функции например:

    -   Управление схемами
    -   Оптимизация для обработки данных

Dataframes имеют схожий API с RDD с дополнительными функциями для big data processing например **filtering**, **aggregation** etc. 

RDD удобно использовать когда нужно больше контроля и гибкости над операциями. Dataframes используются когда нужно работать со структурированными или слабоструктурированными данными и когда мы хотим использовать преимущества от оптимизации Catalyst (про него позже). В целом лучше использовать Dataframes потому что они дают плюс в произодительности. RDD можно юзать только когда нужна гибкость и мы работает с неструктурируемыми данными вроде txt



## Что такое DAG и как это связано со спарком

DAG (Направленный Ациклический Граф) в спарке - это штука которая показывает путь RDD от начала до конца и какие операции с ними происходят. 

## Разница между действиями и трансформациями

Трансформация всегда возвращает новый Dataset, DataFrame, RDD. Если это возвращает что то другое или вообще ничего - это действие 

Кроме того трансформация - это **ленивая** штука которая **не приводит к выполнению**. Спарк просто запомнит что нужно сделать, а **действие** как раз **запускает** трансформацию. 

## Разница между узкими и широкими трансформациями

### Узкие трансформации 

Узкие трансформации работают только в рамках каждого партишна не касаясь других. Количество входящих партишнов = количеству исходящих.  

-   Как правило такие трансформации быстрые. 

-   Не вызывают шафла = нет перехода по стейджам 

-   Map() & filter() - относятся к узким трансформациям 

<p align="center">
<img src="/../main/Questions_for_interview/Images/narrow-transformation.png" width="80%"></p>

### Широкие трансформации

Этот тип преобразования будет иметь входные партиции, которые могут относиться ко многим выходным партициям. Каждый партишн в родительском RDD может использоваться разными партишнами в дочернем RDD. Всегда вызывает **шафл данных**.

-   Медленные, скорость зависит от необходимых операций. Как было сказано выше - вызывает шафл данных. 

-   GroupByKey(), aggregateByKey(), aggregate(), join(), repartition() - примеры функций 

<p align="center">
<img src="/../main/Questions_for_interview/Images/wide-transformation.png" width="80%"></p>

## Что такое job, stage, task

-   Таск - выполняющаяся задача в спарке которая хранит в себе инструкции чтение данных, фильтрация итд). Нужно запомнить что **одна таска - одна партиция** 

-   Stage - каждая стейджа хранит в себе таски и каждая таска выполняет одинаковый сет инструкций. Прикол в том что **при шафле** условие перехода между стейджами - **обязательное**.  

-   Job - просто хранит в себе стейджи. Новая джоба создается когда запускается функция как **write()**. И джоба как раз и является действием, которое запускает трансформации = вариации всех стейджей и их преобразований. 

-   Application - хранит в себе джобы 


## Виды джойнов(не лефт, райт и иннер, а именно стратегий джойна в спарке)

В спарке есть 5 механизмов джойнов: 

-   Перемешанный хеш (Shuffle Hash Join) 

-   Широковещательный хеш (Broadcast Hash Join) 

-   Сортировка через слияние (Sort Merge Join) 

-   Декартов джойн (Cartesian Join) 

-   Широковещательный джойн вложенного цикла (Broadcast Nested Loop Join) 

### Broadcast Hash Join 

**Один** из двух входных наборов данных **транслируется всем исполнителям**. **Хеш-таблица** строится для **всех исполнителей из транслируемого набора данных**. Затем **каждый** партишн **не транслируемого набора** присоединяется независимо к другому набору данных. Он **не требует** этапа **перемешивания**. Единственное **требование** - это чтобы исполнители имели **достаточно памяти** для размещения транслируемого набора данных. Поэтому лучше избегать такого джойна если данные большие. 

### Shuffle Hash Join 

Предварительно **выравнивает** оба набора данных в соответствии со схемой разделения. Если один или оба набора **не соответствуют схеме** - используется **шафл** перед выполнением джойна. После обеспечения соответствия - используется хеш джойн из примера выше. **Требования к памяти** в этом подходе **меньше** чем в Broadcast Hash Join. Это связано с тем тчо тут хеш таблица строится на меньшем наборе входных данных. При этом эффективность будет ниже, если потребуется сделать шафл для одного или обоих входных наборов данных 

### Sort Merge Join 

Как и в шафле оба набора **выравниваются** в соответствии со схемой и в случае **несоответствия** - **шафлятся**. После обеспечения соответствия - используется стандартный подход Sort Merge Join. Он **менее эффективен чем хеширующие джойны**, но гораздо **ниже требования** по **памяти**. Как и в шафлящем - при операции перемешивания больше грузится на выполнение. 

### Cartesian Join 

Используется **исключительно для выполнения перекрестного джойна** (декартово произведение) который выводит все объединенные записи которые  возможны при объединении каждой записи из одного набора входных данных с каждой записью из другого набора входных данных. Сильно увеличивает количество выходных разделов.  

### Broadcast Nested Loop Join 

Один из наборов транслируется всем исполнителям. После этого каждый раздел не транслируемого набора джойнится к транслируемому с помощью стандартной процедуры. **Неэффективен** так как требует **вычислений циклом** + требует **много памяти** для трансляции. 

## Когда какой джойн юзать

Вообще есть такая штука как AQE которая сама решает когда какой джойн применить 

Spark выбирает конкретный механизм для выполнения операции Join, основываясь на следующих факторах: 

-   Параметры конфигурации 

-   Подсказки для Join (можно вручную ставить хинты)

-   Размер наборов входных данных 

-   Тип Join 

-   Эквивалентные или неэквивалентные джойны (Equi or Non-Equi Join) 

## Почему нельзя писать бездумно collect и прочие такие штуки

Коллект собирает **данные** со **всех RDD** со **всех нод** и возвращает на **драйвер**. Это норм применять его на маленьких датасетах, но если юзать его на больших датасетах то скорее всего получишь **OOM**. 

## Как партиционируется файл при загрузке в Spark

Когда мы читаем файл в RDD/Dataframe в Spark данные автоматически распределяются по нодам кластера. Вообще дефолтное количество партиций определяется в настройках Spark кластера параметром ```spark.default.parallelism``` в файле ```conf/spark-defaults.conf``` и является **количеством ядер** в каждом воркере. Поэтому когда мы грузим файл в спарк и значение ```spark.default.parallelism``` не изменялось, то Spark создаст с дефолтным количеством партиций равному количеству ядер драйвера(=общее количество ядер в кластере).

Кроме того, еще стоит учитывать что количество партиций может измениться при чтении большого файла так как системы (например HDFS) хранят данные в блоках с фиксированным количеством памяти (например 128MB). Поэтому когда мы читаем файл 100TB который разбит на блоки по 128MB, то мы получим около 819 200 партиций.

Если мы хотим указать собственное количество партиций то нужно использовать ```repartition()```, например:

```
# Read a large CSV file and repartition it into 200 partitions
df = spark.read.csv("/path/to/data.csv").repartition(200)
```

Это разделит файл на 200 партиций которые будут распределены по нодам.

## Как партиционировать сжатый файл в Spark

Когда мы читаем сжатый файл в Spark, он начинает читать файл чанками и каждый чанк делит на партиции. Можно использовать ```repartition()``` для указания количества партиций при прочитении сжатого файла. Например:
```
# Read a compressed CSV file and repartition it into 8 partitions
df = spark.read.format("csv").option("inferSchema", "true").option("header", "true").option("codec", "gzip").load("/path/to/data.csv.gz").repartition(8)
```
Это разделит сжатый CSV на 8 партиций используя дефолтную стратегию партиционирования Spark (hash partitioning). Вы сами можете указать другую стратегию партиционирования.

## Разница между repartion и coalesce(просто ответа что coalesce ток уменьшает а repartition может и увеличивать мало.Логику того как под капотом это работает. 

Сейчас будет затронута тема оптимизации и зачем вообще использовать **repartition**. Ее можно опустить и перейти сразу к [ответу на вопрос](https://github.com/egorkapot/Innowise_Spark_task/tree/main/Questions_for_interview#%D1%80%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0-%D0%BC%D0%B5%D0%B6%D0%B4%D1%83-repartition-%D0%B8-coalesce) в чем же разница.  

### Зачем использовать repartition 

Представьте что у вас есть файл в котором 150к строк. Весь файл помещается в **один блок данных** и это означает что работать с ним будет только **один воркер**. И если на входе может быть 2МБ то на выходе уже 1ГБ. И это все будет делать **один поток**.  

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartition_vs_coalesce.png" width="80%"></p>

Мы знаем что количество блоков в **одном этапе** неизменно. В таком случае мы можем разорвать этап на два, добавив **repartition(N)**, который **делает шафл**, создавая на выходе **N блоков**, примерно равных по размеру. И так как он делает **шафл** - создается **новый этап**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartititon_new_stage.png" width="80%"></p>

Дальше нам нужно записать готовый результат в директорию, но понимаем что после перетасовки у нас 60 блоков и так как один блок выполняется одним потоком то у нас будет записано 60 файлов. В таком случае разработчики применяют метод **coalesce(N) который в отличие от repartition не приводит к перетасовке = не вызывает новый этап**, а просто складывает наши блоки в N количество блоков данных. Это приведет к тому что на этом этапе у нас будет **N воркеров**. И если мы решим записать все в **один файл**, то у нас будет всего **один воркер**, то ему будет **тяжело** все это **вычислять**. В таком случае перед сохранением с coalesce делают repartition, чтобы разбить последний этап на два: 

1.   Предпоследний который будет выполнять тяжелые вычисления 

2.   Последний который будет произведено сохранение нужных файлов 

#### Ложка дегтя и как фиксится 

Мы знаем что repartition распределяет данные **случайным образом**. И зная что паркет это колоночный формат для хранения файлов, в котором они еще и **сжимаются** - мы **ухудшаем сжимаемость данных**. Это **фиксится** добавлением **порядка** но внутри каждого блока данных 

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartition_order.png" width="80%"></p>

Так как **сортировка** идет внутри **каждого блока**, то **перетасовки нет** и все работает в рамках одного исполнителя памяти. После такой сортировки **размер** исходного файла будет **меньше** так как мы улучшили сжимаемость 

### Разница между Repartition и Coalesce 

-   Repartition - это полная перетасовка = всегда вызывает шафл = новый стейдж 

-   Coalesce - складывание блоков и не вызывает новый шафл = не создает новый стейдж 

## Как работает partitionBy

Тут нужно рассказать о том как работает repartition в том числе. 

### Repartition 

```Repartion(numPartitions, partitionExprs)``` принимает в себя количество партиций, а также колонку, которую в будущем будет использовать для хеширования. Зачем это нужно? Обычный repartition без указания колонки записывает **все файлы в одну директорию** в количестве равном количеству партиций. Если же использовать ```partitionExprs(column)``` то партиции будут осуществляться путем хеширования что означает что все записи для каждого значения колонки будут лежать в одном файле 

### PartitionBy

В ```partitionBy(colNames : String*)``` мы просто прокидываем названия колонок, по которым будет происходить партиция. Причем для каждого значения партиции будет создана отдельная субдериктория.

## PartitionBy в комбинации с repartition или coalesce

Как было сказано выше, **repartition создает партиции**, прокидывая **количество партиций**, а **partitionby** делает партиции по **количеству уникальных значений колонки**. Если использовать их **вместе**, то **сначала** партишнбай сделает **папки для каждого значения колонки**, а репартишн потом **разделит файл на количество нужных партиций**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/partition_by.png" width="80%"></p>


## Как уменьшить количество shuffle

Shuffle - это перетасовка данных между партициями и в этом процессе каждый маппер (M) отправляет задачу каждому редьюсеру (R) что приводит к созданию большого количеству (M*R) файлов, что может привести к большим расходам. Есть много техник как оптимизировать шафлы (тип фильтровать, уменьшить колво данных итд) но мы сейчас говорим про количество самих шафлов. 

### spark.sql.shuffle.partitions 

Spark SQL shuffle это механизм отвечающий за распределение и репартицию данных и с помощью него мы можем увеличить или уменьшить количество партиций в RDD/DF используя ```spark.sql.shuffle.partitions```. По дефолту это значения равно 200, но в этом нет смысла если у нас немного данных. Поэтому можно установить меньшее значение, чтобы уменьшить количество шафлов.  

Кроме того, в зависимости от этого значения, спарк выбирает какой шафл применять. При значении ```spark.sql.shuffle.partitions<200``` дефолтным будет механизм **hash shuffle**. При ```spark.sql.shuffle.partitions>200```, то sort **based shuffle** 

### Broadcast Hash Join 

Когда мы джойним два датасета и один из них **достаточно мал**, чтобы **поместиться** в память **одного исполнителя**, то он грузится в **хеш-таблицу** на **драйвер** и транслируется каждому исполнителю. И тогда меп трансформация может обращаться к этой хеш-таблице чтобы лукапить данные 

### Вместо repartition использовать coalesce 

В примере выше я говорил что repartition всегда делает шафл, в отличие от coalesce, который будет стараться объединить блоки внутри одного исполнителя. 

### Использовать aggregateByKey() вместо groupByKey() 

Вообще ```groupbykey()``` опасно юзать так как он часто вызывает проблемы с памятью. Что он делает? Он сначала шафлит все значения а потом их мерджит что влияет на производительность. Поэтому мы можем юзать ```reducebykey``` или ```aggregatebykey``` (они отличаются только тем что в агрегат можно прокинуть начальные параметры и логику объединения). Как они работают? Сначала вызывается **комбайнер** который **мерджит** все значения по ключам и только **потом запускает шафл** что дает больший перфоманс 


## Оптимизации которые юзает каталист

К узлам дерева Catalyst применяет набор правил для их оптимизации, которая выполняется в четыре этапа, как показано на диаграмме ниже. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/catalyst.png" width="80%"></p>

### Анализ 

Когда к нам прилетают данные каталист **не знает вообще ничего** о них, ни о типах данных, названий колонок итд. Поэтому когда мы отправляем запрос, спарк использует каталист чтобы **определить тип** каждого передаваемого столбца и **существуют ли они вообще**. Сначала каталист **создает дерево** для **неразрешенного логического плана** и затем **применяет** свои **правила** чтобы определить все **ссылки** на атрибуты и отношения. Для этого он использует объект **Catalog** который отслеживает таблицы во всех источниках данных 

### Логическая оптимизация 

После версии 2.2 представили оптимизатор затрат который использует **статистику** и **мощности** машины для поиска наиболее **эффективного** плана выполнения вместо простого набора правил. На этом этапе каталист оптимизирует затраты и **готовый план** кидает в **следующий этап** 

### Физический план 

Генерирует **несколько** физических планов для выбора **наиболее эффективного** из предложенных 

### Генерация кода 

Этап создания **байт-кода Java** для запуска на каждой машине. Для этого используется функция Scala – QuasiQuotes для преобразования дерева задания в абстрактное синтаксическое дерево (AST) которое компилирует и запускает код 

## Как решить проблему Data Skew и вообще что это такое

Data Skew - это **перекос** данных, другими словами это когда *данных* в одной партиции гораздо **больше** чем в других (это может быть вызвано шафлом, например группировкой). Почему это плохо? Спарк может закинуть эту партицию в **оперативку** и тогда все будет работать очень **медленно**, а если памяти не хватит - OOM 

### Broadcast Hash Join  

Обсуждали этот тип джойна выше. Из **наименьшего** датафрейма делается **хеш-таблица** и кидается на **драйвер**, драйвер **соединит** кусочки хеш-таблицы с **разных работников** и потом отправит эту **хеш-мапу** на **всех** работников. Не рекомендуется кидать большие файлы на драйвер, это может плохо кончится. Кроме того AQE может решить делать этот джойн или нет, если нужно спровоцировать это напрямую, то нужно указать механизм джойна с помощью хинта 
 
### Хеширование с солью (salting) 

Добавление рандомных (в плане мы сами выбираем что добавить) данных которые к ключу, для того чтобы получить **равномерное распределение ключей** для хеша. 

В спарке делается так: 

1.  Создаем колонку с рандомными значениями 

2.  Комбинируем новую колонку с существующими ключами для получения композитного ключа и делаем любую трансформацию 

3.  Группируем финальный результат 

<p align="center">
<img src="/../main/Questions_for_interview/Images/hashing_with_salt.png" width="80%"></p>

### AQE 

В случае с Data Skew работает с **sort merge join**. После первого шафла собирает **статистику** и решает что лучше сделать. Например если одна из партиций будет слишком большой то AQE может сам засолить эту партицию и сделать несколько партиций поменьше. 


## UDF и почему PySpark UDF это плохо. Чем заменить PySpark UDF

Когда не хватает встроенного функционала, на помощь приходят UDF. По сути обычная функция, только для работа с колонками. Есть три вида: PySpark UDF, Pandas UDF, Scala UDF. В целом очевидно самая **быстрая Scala UDF**, на втором месте **Pandas UDF** так как она работает с **векторами**, ну и самая медленная **PySpark UDF** так как он работает **поэлементно**. На Scala очевидно вы будете юзать только Scala UDF, но на PySpark чаще всего PySpark UDF. Тем не менее, если выполнение UDF становится проблемой(по производительности), то стоит **заменить на Pandas UDF**. Ну а вообще лучше их не юзать, ибо оптимизитор их не оптимизирует(он же не знает что вы там напишите), да и те же PySpark UDF реально медлительны. 


## Delta Lake и почему он

Сам **Data Lake** не позволяет делать операции **merge, update, delete** итд потому что файлы там **не поддерживают ACID**. Решением является добавление **JSON** файла который **хранит** в себе **лог информацию** вроде загрузок, актуальных данных итд. Благодаря этому наши **файлы совместимы с ACID** правилами и с помощью Delta API мы можем делать update, delete, merge и даже смотреть **прошлые файлы** тк мы видим лог и знаем когда что добавлялось, обновлялось итд. Причем Delta API не удаляет файлы при обновлении как вы поняли, он просто хранит файлы и логи с ними. Для удаления нужно использовать ```vacuum```. 

## Менеджмент памяти на уровне JVM+overhead

До спарка 1.6 механизм управления памяти был другим и определялся классом StaticMemoryManager. С версией 1.6 и выше контроль памяти на себя взял UnifiedMemoryManager класс. 

Начнем с того, что основным языком реализации Спарка является Scala поэтому все операции ранятся на JVM. Среда выполнения это **Heap** (куча), которая делится на 4 (Spark Memory состоит из Storage Memory и Execution Memory) разные части.  

<p align="center">
<img src="/../main/Questions_for_interview/Images/jvm_heap.png" width="80%"></p>

Кроме того есть еще два сегмента памяти к которым обращается Спарк: 

-   Память вне кучи (Off-Heap Memory) - сегмент за пределами JVM который иногда используется например для метода ```intern()```, который гарантирует что все строки с одинаковым содержимым совместно используют одну и ту же память. Кроме того память вне кучи может хранить сериализованные датафреймы и RDD. 

-   Внешняя память процесса (External Process Memory) - эту память используют програмы PySpark, SparkR которые ранятся на Python и R вне JVM. 

Так же надо сказать про две основные конфигурации для контроля размещения памяти в хранилище (так их 10 для настройки управления памятью но для хранилища только 2): 

-   ```spark.memory.fraction (defaults to 0.75)``` - сегмент от 300МБ для выполнения **хранения** данных.  Чем оно **меньше** тем **чаще** происходит **утечка** и **вытеснение** кэшированных данных. Позволяет выделить память для внутренних метаданных и структур пользовательских даннных. 

-   ```memory.storageFraction (defaults to 0.5)``` - часть spark.memory.fraction, обьем памяти хранения который невозможно вытеснить. Чем больше это значение тем меньше оперативной памяти доступно для выполнения и задачи чаще сохраняются на диск. 

Теперь можно поговорить про разделение памяти в Heap: 

### Reserved Memory (зарезервированная память)  

Зарезервирована системой и с версии 1.6 составляет 300МБ. Эта память хранит внутренние обьекты спарка. Прикол в том что если память исполнителя будет меньше чем в 1.5 раза зарезервированной памяти, то спарк даст ошибку "Please use larger heap size" 

Формула подсчета: Дефолтное значение в 300МБ 

### User Memory (пользовательская память)  

Для хранения структур данных созданных **пользователем**, UDF итд. Этот сегмент памяти не управляется спарком. 

Формула подсчета: (Heap (сколько у нас всего памяти в куче) - Reserved Memory (зарезервированная память)) * (1.0 -  ```spark.memory.fraction``` (это сколько мы отдаем в саму память спарка))  

### Storage Memory (память хранилища) 

Зарезервированная память для кэширования данных. Спарк **чистит** место для **новых** файлов путем **удаления старых** кешированных файлов по механизму **LRU** (Least Recently Used). Когда кешированные данные заполнили диск они будут или записаны на диск или пересчитаны на основе кофигурации. 

Формула подсчета: Storage Memory = (Heap – Reserved Memory) * ```spark.memory.fraction``` * ```spark.memory.storageFraction``` (определяет какой процент от спарковской памяти мы отдаем под хранение) 

### Execution Memory (память выполнения) 

Используется для хранения объектов созданных во время выполнения **тасков**. Как прави512МБо используется для хранения **хеш-таблиц** или для хранения **буфера шафла**. Так же поддерживает **spilling** на диск при недостатке памяти. 

Формула подсчета: (Heap – Reserved Memory) * ```spark.memory.fraction``` * (1.0 - ```spark.memory.storageFraction``` (то есть остаток от части спарковской памяти которую мы отдали под хранение) 

### Исполнители и памяти вне кучи 

Помимо использования памяти в куче JVM исполнитель может обращаться к внешнему пространству через API интефейсы ```sun.misc.Unsafe``` Память вне кучи находится за пределами сборки мусора поэтому предоставляет разработчику более точный контроль над памятью. Спарк использует память вне кучи для работы с памятью за счет метода ```string.intern()```. Так же память вне кучи нужна для хранения данных в рамках проекта **Tungsten** (это такая штука которая работает сразу с сериализованным бинарными данными) который повышает эффективность операций обработки данных. 

Общая память вне кучи контролируется ```spark.executor.memoryOverhead``` который по дефолту равен 10% памяти исполнителя но не ниже 384МБ. Этот объем выделяется для каждого исполнителя и увеличивается вместе с исполнителем. Поддерживается в **YARN** и **Kubernetes** 

Эта память так же включает в себя память исполнителя PySpark (если ```spark.executor.pyspark.memory``` не настроен отдельно). 

## Менеджмент памяти на полном уровне(если это PySpark то там не ток JVM и overhead)

К статье выше можно добавить что при выполнении кода на PySpark используются участки памяти заданные через ```spark.python.worker.memory``` и ```spark.executor.pyspark.memory``` При запуске кода пользователя исполнитель ранит два отдельных процесса, которые взаимодействуют друг с другом через мост Py4J: 

-   JVM - выполняет часть кода Спарка который связан с шафлом 

-   Python - ранит сам код пользователя 

 
Параметр ```spark.python.worker.memory```  управляет объемом памяти зарезервированным для **каждого процесса** воркера PySpark за пределами которого он **переносится** на **диск** то есть этот объем памяти **может быть занят объектами созданными через мост Py4J** во время Спарк операций. Если параметр не установлен то по **дефолту** будет **512МБ**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/pyspark_memory.png" width="80%"></p>

```spark.executor.pyspark.memory``` контролирует фактическую память процесса воркера Python, устанавливая предел пространства памяти который он может адресовать с помощью свойства ```system.RLIMIT_AS```. Если память worker’а Python не установлена через параметр ```spark.executor.pyspark.memory```, этот процесс потенциально **может занять всю память узла**. А, поскольку эта часть памяти не отслеживается диспетчером ресурсов Спарк-кластера, таким как Hadoop YARN, есть **риск перепланирования** в узле и смены страниц в памяти. В результате возможно **замедление** работы всех **контейнеров** YARN на этом узле. 


## Parquet - что это(рассказать всё что знаешь)

Бинарный **колоночно-ориентированный** формат для хранения больших данных, который очень хорошо оптимизирован для спарка, так как спарк **может считывать мету и ему не нужно делать лишние вычисления**. Паркет использует архитектуру основанную на "**уровнях определения**" - определяют количество необязательных полей в пути для **столбца** и "*уровнях повторения*" - указывают для какого повторяемого **поля** в пути значение имеет повторение. Паркеты делятся на несколько уровней что позволяет осуществлять параллельное выполнение операций: 

### Row-group 

Горизонтальное разбиение на строки для работы на **уровне Map-Reduce**. Используются для реализации коцепции локальности данных, когда **каждый узел** кластера **считывает** лишь ту *информацию*, которая хранится непосредственно **на его жестком диске** 

### Column chunk  

Колоночное разбиение где **блок данных из столбца** находится в определенной группе **строк**. То есть в строке данные будут идти подряд что **облегчает их чтение** по сравнению с csv. Благодаря этому мы можем считывать только нужные нам колонки. Нужно для распределения IO операций. 

### Page 

Разделение по страницам для распределения работ по кодированию и сжатию. Каждая колонка содержит в себе Pages которые содержат метаинформацию и закодированные данные 
 
### Преимущества 

-   Могут храниться не только в hdfs, но и в других файловых системах как GlusterFs или поверх NFS 

-   Это файлы поэтому легко перемещать, реплицировать итд 

-   Можно делать быстрый анализ так как мета + колончатый вид 

-  Хорошо сжимаются (хуже ОРС но выигрывает по скорости) 

-   Самое быстрое чтение (даже  всравнении с ОРС) 

### Недостатки 

-   Колончатый вид заставляет задумываться о схеме и типах данных. 

-   Кроме как в Spark, Parquet не всегда обладает нативной поддержкой в других продуктах. 

-   Не поддерживает изменение данных и эволюцию схемы. Конечно, Spark умеет мерджить схему, если у вас она меняется со временем (для этого надо указать специальную опцию при чтении), но, чтобы что-то изменить в уже существующим файле, нельзя обойтись без перезаписи, разве что можно добавить новую колонку. 

-   Не поддерживаются транзакции, так как это обычные файлы а не БД. 

## Нюансы при чтении JSON

JSON - это **не splittable** файл, то есть спарк не может разделить его для параллельной обработки потоками. Поэтому если нам прилетит джейсонина размером 1ГБ то для спарка это будет одна партиция что приведет к **spill-эффекту** (мы знаем что спарк работает с оперативкой, но при spill эффекте он переливает данные на диск. Spill = перемещение RDD из оперативки на диск и обратно. Что еще хуже, то может быть OOM. Вообще есть разработка(ток не помню от кого) расширения для JSON, который является splittable, но думаю вы его не встретите. 

## Нюансы при чтении CSV

CSV хранит файлы в **строках**, что означает что при **прочтении** файла нужно будет прочесть **все строки**. Да он **будет удалять ненужные данные** потом, но все равно идет **большая нагрузка на диск** так как ему нужно считать все. В спарке есть оптимизаторы **predicate pushdown (строки)** / **projection pushdown (колонки)** которые во время чтения строк в CSV сразу применяют фильтр. То есть спарк читает строку и если она не подходит под фильтр - удаляет (можно не удалять прокинув перзист и кешед). Кроме того CSV **не хранит мета информацию** (как паркет например) поэтому спарку нужно будет самому все считать и определить. Как я понял это **нагружает драйвер** потому что при паркете он получает мета информацию и ему легче распределить ресурсы. 

## Broadcast variables

Когда мы используем какую то переменную при трансформации то эта переменная отправляется на исполнение вместе с задачей. Мы помним что **одна задача - одна партиция** и получается что нам нужно отправить 500 переменных для трансформации 500 партиций. Звучит так себе особенно если учитывать что в качестве переменной могут быть огромные массивы. Для этого можно закинуть переменную в броадкаст = послать ее в конкретный раздел памяти где она будет транслироваться всем партициям (напоминает механизм броадкаст джойна да?) 